from transformers import GenerationConfig, AutoModelForCausalLM
import torch

class ResponseGenerator:
    """ä½¿ç”¨åŸç”Ÿæ¨¡å‹å›å¤ç”Ÿæˆæ¨¡å—"""

    GEN_CONFIG = {
        "max_new_tokens": 512, # é™åˆ¶ç”Ÿæˆå†…å®¹æœ€å¤š256ä¸ªæ–°Token
        "temperature": 0.7, # æ§åˆ¶éšæœºæ€§ï¼ˆå€¼è¶Šä½è¾“å‡ºè¶Šç¨³å®šï¼‰
        "top_p": 0.9, # æ ¸é‡‡æ ·ï¼ˆåªä¿ç•™æ¦‚ç‡ç´¯è®¡å‰90%çš„Tokenï¼‰
        "top_k": 50,  # æ–°å¢top-ké‡‡æ ·
        "repetition_penalty": 1.2, # æƒ©ç½šé‡å¤å†…å®¹ï¼ˆå¤§äº1æ—¶æŠ‘åˆ¶é‡å¤ï¼‰
        "do_sample": True, # å¯ç”¨é‡‡æ ·
        "num_beams": 1,  # æ˜¾å¼å…³é—­æŸæœç´¢
        # "pad_token_id": 0,  # æ˜¾å¼æŒ‡å®šå¡«å……token
        # "eos_token_id": 2  # æ˜¾å¼æŒ‡å®šç»“æŸtoken
    }

    def __init__(self, model_name, tokenizer):
        # åŠ è½½æ¨¡å‹
        self.tokenizer = tokenizer
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.model = self.model.to('cuda') if torch.cuda.is_available() else self.model
        self.gen_config = GenerationConfig(**self.GEN_CONFIG)
        # å¤„ç†å¯èƒ½çš„pad_tokenç¼ºå¤±é—®é¢˜
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token


    def generate(self, history):
        """ç”Ÿæˆæ•™å¸ˆå›å¤"""
        system_msg = {"role": "system", "content": "You are a mathematics tutoring assistant. Your role is to guide students through Socratic questioning."} # ç³»ç»Ÿå›ºå®šæç¤º
        full_conversation = [system_msg] + history
        # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
        inputs = (self.tokenizer.apply_chat_template( # å°†å¯¹è¯è½¬åŒ–ä¸ºæ¨¡å‹æ‰€éœ€æ ¼å¼
            full_conversation,
            add_generation_prompt=True, # åœ¨æœ«å°¾æ·»åŠ åŠ©æ‰‹æ ‡è®°
            return_tensors="pt", # è¿”å›PyTorchå¼ é‡
            truncation = True,  # æ·»åŠ æˆªæ–­
            max_length = 2048  # æ§åˆ¶è¾“å…¥é•¿åº¦
        ).to(self.model.device))

        outputs = self.model.generate(
            inputs,
            generation_config=self.gen_config
        )

        return self.tokenizer.decode(
            outputs[0][inputs.shape[1]:],  # ç§»é™¤è¾“å…¥ï¼Œåªä¿ç•™æ–°ç”Ÿæˆçš„Token
            skip_special_tokens=True,  # è·³è¿‡ç‰¹æ®ŠToken
            clean_up_tokenization_spaces = True  # æ¸…ç†å¤šä½™ç©ºæ ¼
        ).strip()  # å»é™¤é¦–å°¾ç©º

    def chat_loop(self):
        """æ§åˆ¶å°å¤šè½®å¯¹è¯äº¤äº’"""
        history = []
        print("\n=== æ•°å­¦è¾…å¯¼å¯¹è¯ç³»ç»Ÿ ===")
        print("è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æƒ³æ³•ï¼ˆè¾“å…¥ 'exit' æˆ–ç›´æ¥å›è½¦é€€å‡ºï¼‰\n")

        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("ğŸ‘¤ ç”¨æˆ·: ")

                # é€€å‡ºæ¡ä»¶åˆ¤æ–­
                if not user_input.strip() or user_input.lower() == "exit":
                    print("\nğŸ”„ å¯¹è¯ç»“æŸã€‚")
                    break

                # å°†ç”¨æˆ·è¾“å…¥åŠ å…¥å†å²
                history.append({"role": "user", "content": user_input})

                # ç”ŸæˆåŠ©æ‰‹å›å¤
                print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...", end="", flush=True)
                response = self.generate(history)
                print("\r", end="")  # æ¸…é™¤æ­£åœ¨æ€è€ƒæç¤º

                # æ˜¾ç¤ºå¹¶è®°å½•å›å¤
                print(f"ğŸ¤– åŠ©æ‰‹: {response}\n")
                history.append({"role": "assistant", "content": response})

            except KeyboardInterrupt:
                print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡ºå¯¹è¯...")
                break

            except Exception as e:
                print(f"\nâŒ ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                print("æ­£åœ¨é‡ç½®å¯¹è¯å†å²...")
                history = []  # é‡ç½®å¯¹è¯ä»¥é˜²é”™è¯¯ç´¯ç§¯