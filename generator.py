from peft import PeftConfig, PeftModel
from transformers import GenerationConfig, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import json
import time
from pathlib import Path
from tqdm import tqdm  # è¿›åº¦æ¡åº“


class ResponseGenerator:
    """ä½¿ç”¨åŸç”Ÿæ¨¡å‹å›å¤ç”Ÿæˆæ¨¡å—"""
    GEN_CONFIG = {
        "max_new_tokens": 128, # é™åˆ¶ç”Ÿæˆå†…å®¹æœ€å¤š256ä¸ªæ–°Token(å¤ªé«˜ç”Ÿæˆå†—ä½™å†…å®¹ï¼Œå¤ªä½è¿‡æ—©æˆªæ–­)
        "temperature": 0.3, # æ§åˆ¶éšæœºæ€§ï¼ˆå€¼è¶Šä½è¾“å‡ºè¶Šç¨³å®šï¼Œå€¼è¶Šé«˜åˆ›æ„æ€§è¶Šå¼ºï¼‰
        "top_p": 0.7, # æ ¸é‡‡æ ·ï¼ˆåªä¿ç•™æ¦‚ç‡ç´¯è®¡å‰90%çš„Tokenï¼Œä½å€¼æ›´é›†ä¸­ï¼Œé«˜å€¼æ›´å¤šæ ·ï¼‰
        "top_k": 30,  # ké‡‡æ ·ï¼Œä»å‰kä¸ªå€™é€‰tokené‡‡æ ·ï¼Œå°kæ›´ä¿å®ˆï¼ˆ10-30ï¼‰ï¼Œå¤§kæ›´å¼€æ”¾ï¼ˆ50-100ï¼‰
        "repetition_penalty": 1.5, # æƒ©ç½šé‡å¤å†…å®¹ï¼Œè½»åº¦1.0~1.2ï¼Œä¸¥æ ¼1.5~2.0
        "do_sample": True, # å¯ç”¨é‡‡æ ·ç­–ç•¥ï¼ŒFalse: è´ªå©ªè§£ç ï¼ˆé€‰å‡†ç¡®æ€§æœ€é«˜ï¼‰
        "num_beams": 3,  # 1ï¼šå•æŸï¼Œ3~5ï¼šè´¨é‡é«˜ä½†é€Ÿåº¦æ…¢
        "early_stopping": True  # é‡åˆ°åˆç†ç»“æœæå‰åœæ­¢
    }

    def __init__(self, model_name, tokenizer, user_lora=False, adapter_path=""):
        # åŠ è½½æ¨¡å‹
        self.tokenizer = tokenizer
        self.tokenizer.pad_token = self.tokenizer.eos_token  # æˆ–ç”¨ç‰¹å®štok
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            offload_folder="./offload",  # æŒ‡å®šå­˜å‚¨å¸è½½æƒé‡çš„æ–‡ä»¶å¤¹
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True,
            use_cache=True, # ç”Ÿæˆæ—¶å¿…é¡»å¯ç”¨ç¼“å­˜
        )

        # æ˜¯å¦ä½¿ç”¨Loraé€‚é…å™¨
        if user_lora:
            # åŠ è½½é€‚é…å™¨é…ç½®
            self.peft_config = PeftConfig.from_pretrained(adapter_path)
            # åˆå¹¶é€‚é…å™¨ï¼Œå°†LoRAé€‚é…å™¨åŠ è½½åˆ°åŸºç¡€æ¨¡å‹
            self.model = PeftModel.from_pretrained(
                self.model,
                adapter_path,
                device_map="auto"
            )

        self.model.eval()  # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼
        self.gen_config = GenerationConfig(**self.GEN_CONFIG)

    def generate(self, history):
        """ç”Ÿæˆæ•™å¸ˆå›å¤"""
        system_msg = {"role": "system", "content": "You are a mathematics tutoring assistant. Your job is to provide students with solutions to math problems."} # ç³»ç»Ÿå›ºå®šæç¤º
        full_conversation = [system_msg] + history
        # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
        inputs = (self.tokenizer.apply_chat_template( # å°†å¯¹è¯è½¬åŒ–ä¸ºæ¨¡å‹æ‰€éœ€æ ¼å¼
            full_conversation,
            add_generation_prompt=True, # åœ¨æœ«å°¾æ·»åŠ åŠ©æ‰‹æ ‡è®°
            return_tensors="pt", # è¿”å›PyTorchå¼ é‡
        ).to(self.model.device))

        with torch.inference_mode():
            outputs = self.model.generate(
                input_ids=inputs,  # ç›´æ¥ä¼ å…¥äºŒç»´å¼ é‡
                generation_config=self.gen_config,
                attention_mask=(inputs != self.tokenizer.pad_token_id)  # åŠ¨æ€ç”Ÿæˆæ³¨æ„åŠ›æ©ç 
            )

        return self.tokenizer.decode(
            outputs[0][inputs.shape[1]:],  # ç§»é™¤è¾“å…¥ï¼Œåªä¿ç•™æ–°ç”Ÿæˆçš„Token
            skip_special_tokens=True,  # è·³è¿‡ç‰¹æ®ŠToken
            clean_up_tokenization_spaces = True  # æ¸…ç†å¤šä½™ç©ºæ ¼
        ).strip()  # å»é™¤é¦–å°¾ç©º

    def chat_loop(self):
        """æ§åˆ¶å°å¤šè½®å¯¹è¯äº¤äº’"""
        history = []
        print("\n=== æ•°å­¦è¾…å¯¼å¯¹è¯ç³»ç»Ÿ ===")
        print("è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æƒ³æ³•ï¼ˆè¾“å…¥ 'exit' æˆ–ç›´æ¥å›è½¦é€€å‡ºï¼‰\n")

        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("ğŸ‘¤ ç”¨æˆ·: ")

                # é€€å‡ºæ¡ä»¶åˆ¤æ–­
                if not user_input.strip() or user_input.lower() == "exit":
                    print("\nğŸ”„ å¯¹è¯ç»“æŸã€‚")
                    break

                # å°†ç”¨æˆ·è¾“å…¥åŠ å…¥å†å²
                history.append({"role": "user", "content": user_input})
                # ç”ŸæˆåŠ©æ‰‹å›å¤
                print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...", end="", flush=True)
                response = self.generate(history)
                print("\r", end="")  # æ¸…é™¤æ­£åœ¨æ€è€ƒæç¤º

                # æ˜¾ç¤ºå¹¶è®°å½•å›å¤(å¸¦æ‰“å­—æœºæ•ˆæœï¼‰
                print("\nğŸ¤– åŠ©æ‰‹ï¼š", end="")
                for char in response:
                    print(char, end="", flush=True)
                    time.sleep(0.02)  # è°ƒæ•´æ‰“å°é€Ÿåº¦
                print()
                history.append({"role": "assistant", "content": response})

            except KeyboardInterrupt:
                print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡ºå¯¹è¯...")
                break

            except Exception as e:
                print(f"\nâŒ ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                print("æ­£åœ¨é‡ç½®å¯¹è¯å†å²...")
                history = []  # é‡ç½®å¯¹è¯ä»¥é˜²é”™è¯¯ç´¯ç§¯

    def file_response(self, test_file, output_dir):
        """æ•°æ®é›†æ‰¹é‡ç”Ÿæˆå›å¤"""
        # è¯»å–æµ‹è¯•é›†
        with open(test_file, "r", encoding="utf-8") as f:
            test_data = json.load(f)  # å‡è®¾æµ‹è¯•é›†æ˜¯JSONåˆ—è¡¨æ ¼å¼
        # æ‰§è¡Œæ‰¹é‡æµ‹è¯•
        results = []
        for idx, question in enumerate(tqdm(test_data, desc="Processing")):
            try:
                start_time = time.time()
                response = self.generate(question["messages"])
                # è®°å½•ç»“æœ
                results.append({
                    "question": question["messages"],
                    "answer": response,
                    "processing_time": time.time() - start_time,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                })

            except Exception as e:
                print(f"å¤„ç†ç¬¬ {idx + 1} é¢˜æ—¶å‡ºé”™ï¼š{str(e)}")
                results.append({
                    "question": question["messages"],
                    "error": str(e),
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                })

        # ä¿å­˜ç»“æœ
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        # åˆ›å»ºç»“æœç›®å½•
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        timestamp = time.strftime("%Y%m%d-%H%M%S") # ç”Ÿæˆæ—¶é—´æˆ³å­—ç¬¦ä¸²
        output_path = f"{output_dir}/results_{timestamp}.json"

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")